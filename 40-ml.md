****
# День 3. Алгоритмы машинного обучения <a name="4"></a>

# Методология машинного обучения <a name="4_1"></a>

Существует мнение о том, что не существует единого метода машинного обучения, который лучше всего справлялся бы со всеми проблемами. Независимо от того, насколько сложный или простой метод, он не будет работать наилучшим образом для всех проблем. 

Поэтому, чтобы найти лучший метод и его алгоритмическую реализацию, которые соответствуют потребностям конкретной задачи, необходимо обладать кругозором и владеть программными средствами для реализации алгоритмов анализа данных.  

Считается, что прежде чем углубляться в сложные методы и тратить время на тонкую настройку модели, лучше попробовать более простые методы и алгоритмы. По мере продвижения к более сложным методам мы можем обнаружить, что для наших нужд оказывается достаточно уже примененнного и более простого подхода.

Гибкая методология машинного обучения предполагают, что мы должны развивать понимание данных итерационно. Это означает, что мы не должны пытаться разрешить все проблемы сразу. Для науки о данных это означает, что мы начинаем с простого подхода и готовимся к применению более сложных методов и методик. Первая итерация должна иметь наиболее простой вариант реализации каждого этапа конвейера машинного обучения(например, обработка, извлечение признаков и пр.). Дополнительным достоинством такого подхода является то, что применение упрощенных подходов менее затратно с точки зрения вычислительной мощьности, не требует интенсивных вычислений или дорогих поисков гиперпараметров. Бесспорно, простая модель может работать плохо, но получение этой модели возможно максимально быстро и с минимальными затратами ресурсов. 

Пример такого простого метода: поиск ближайшего соседа и другие подобные методы. Для их реализации требуется лишь несколько строк кода. Вместе с тем нет никаких причин, по которым более простые методы не могут быть лучше сложных. 

На последующих итерациях конверйера машинного обучения исследуются другие подходы. Это позволит нам сравнить, как методологические изменения влияют на производительность, и отслеживать улучшения с течением времени. Тем не менее, по мере дальнейших исследований улучшение качества модели становится все более проблематичным. Например, если мы достигли точности 99%, возможно, нам не следует тратить больше времени и ресурсов, чтобы пробовать доводить ее до 99,2%. 


# Предварительная обработка данных <a name="4_1_1"></a>


Целью предварительной обработки является преобразование необработанных данных в форму, которая подходит для машинного обучения. Структурированные и чистые данные позволяют получать более точные результаты из прикладной модели машинного обучения. Включает форматирование данных, очистку и выборку. Этот этап может также включать в себя сокращение числа несвязанных признаков с помощью разработки признаков, если у нас есть люди, которые могут это сделать (для этого требуются расширенные знания предметной области). Это будет важно, чтобы уменьшить сложность моделей.

Очистка данных, это набор процедур позволяет удалить шум и устранить несоответствия в данных. Это включает в себя заполнение недостающих данных с использованием методов вменения, например, замена отсутствующих значений на средние или максимальные частые значения. Обнаружение выбросов также важно (наблюдения, которые значительно отличаются от остальной части распределения). Если какие-либо выбросы указывают на ошибочные данные, их следует удалить или исправить, если это возможно. Этот этап также включает в себя удаление неполных и бесполезных записей данных и столбцов.

Масштабирования – данные могут иметь числовые атрибуты (признакы), которые охватывают различные диапазоны, например, миллиметры, метры и километры. Масштабирование - это преобразование этих атрибутов, чтобы они имели одинаковый масштаб, например, от 0 до 1 или от 1 до 10 для наименьшего и наибольшего значения для атрибута. Мин-макс нормализация вычитает минимальное значение в объекте, а затем делит на диапазон объекта. Он сохраняет исходное распределение функции и не вносит существенных изменений в информацию, встроенную в исходные данные. С другой стороны, стандартизация признака стандартизирует признак путем вычитания среднего значения, а затем деления на стандартное отклонение. Это изменяет распределение признаков и приводит к распределению со средним значением, равным нулю, и стандартным отклонением, равным единице. Логарифмическое преобразование важно для преобразования мультипликативных отношений между признаками в аддитивные отношения. Так как большие значения уменьшаются больше, чем маленькие, логарифмическое преобразование имеет эффект псевдоскейлинга, поскольку различия между большими и малыми значениями в наборе данных уменьшаются. Выбор одного метода масштабирования среди других зависит не только от набора данных, но и от выбранного метода машинного обучения, поскольку различные методы машинного обучения фокусируются на разных аспектах данных. Например, метод кластеризации фокусируется на анализе сходства точек данных, в то время как анализ главных компонентов объясняет как можно больше вариаций данных с минимально возможным количеством компонентов. Использование одного метода масштабирования может улучшить результаты кластеризации, скрывая результаты анализа PCA.

Уменьшение размерности (Dimesion Reduction) – основная проблема при интерпретации многомерных данных заключается в том, что соответствующие, истинные сигналы скрыты нерелевантными данными или шумом. Для данных низкого и среднего размера можно использовать несколько классических статистических методов, которые фокусируются на идентификации сильных, истинных сигналов. Такие методы, однако, не могут быть распространены на случаи, когда размерность данных намного превышает размер записей, а слабые истинные сигналы окружены значительным количеством шума (как в случае современного геномного анализа). Кроме того, диапазон шума имеет тенденцию увеличиваться с увеличением размерности данных, что часто делает существующие методы непрактичными [3]. По мере роста количества признаков или измерений объем данных, которые нам необходимы для точного обобщения, растет в геометрической прогрессии.

При добавлении нового признака в модель иногда не хватает данных для поддержания отношений, и, следовательно, новый признак может не иметь положительное влияние на модель. Поэтому мы заканчиваем тем, что делаем его более сложным, не используя его в своих интересах. Например, в нашем исследовании мы имеем 299 переменных (p = 299). В этом случае мы можем иметь 299 (299-1) / 2 = 44551 разных участков. Нет смысла визуализировать каждый из них в отдельности, верно? В тех случаях, когда у нас большое количество переменных, лучше выбрать подмножество этих переменных (p << 100), которое собирает столько информации, сколько и исходный набор переменных. Вот некоторые преимущества применения уменьшения размерности к набору данных:
    • пространство, необходимое для хранения данных, уменьшается по мере уменьшения количества измерений,
    • меньше размеров приводит к меньшему времени вычислений / обучения,
    • некоторые алгоритмы не работают хорошо, когда у нас большие размеры. Таким образом, чтобы эти алгоритмы были полезны, необходимо уменьшить эти размеры,
    • он заботится о мультиколлинеарности, удаляя лишние признакы. Например, у вас есть две переменные - «время, проведенное на беговой дорожке в минутах» и «сожженные калории». Эти переменные сильно коррелируют: чем больше времени вы проводите на беговой дорожке, тем больше калорий вы будете сжигать. Следовательно, нет смысла хранить оба, так как только один из них делает то, что нужно,
    • это помогает в визуализации данных. Как обсуждалось ранее, очень трудно визуализировать данные в более высоких измерениях, поэтому сокращение нашего пространства до 2D или 3D может позволить нам более четко отображать и наблюдать шаблоны.

Уменьшение размерности может быть сделано двумя различными способами:
    • выбор признаков: сохраняются только самые важные переменные из исходного набора данных,
    • уменьшение размерности : путем нахождения меньшего набора новых переменных, каждая из которых является комбинацией входных переменных, содержащих в основном ту же информацию, что и входные переменные.

Алгоритмы выбора признаков – один из наиболее широко используемых алгоритмов выбора объектов. Это помогает выбрать меньшее подмножество признаков. Причина в том, что древовидные стратегии, используемые случайными лесами, естественным образом ранжируются по тому, насколько хорошо они улучшают чистоту узла. Это означает уменьшение примесей по всем деревьям (так называемая примесь Джини). Узлы с наибольшим уменьшением примесей происходят в начале деревьев, тогда как примечания с наименьшим уменьшением примесей происходят в конце деревьев. Таким образом, обрезая деревья ниже определенного узла, мы можем создать подмножество наиболее важных функций. Случайный лес состоит из нескольких деревьев решений. Каждый узел в деревьях решений является условием для отдельного объекта, предназначенного для разделения набора данных на два, так чтобы одинаковые значения ответов оказались в одном наборе. Мера, на основе которой выбирается (локально) оптимальное условие, называется примесью.

Обратное Удаление признаков (Backward Feature Elimination) -Удаление признаков рекурсивно удаляет признакы, строит модель с использованием оставшихся признаков и вычисляет точность модели. Он включает в себя следующие этапы:
        1) сначала мы берем все «n» переменных, присутствующих в нашем наборе данных, и обучаем модель, используя их,
        2) затем мы рассчитываем производительность модели,
        3) теперь мы вычисляем производительность модели после исключения каждой переменной (n раз), то есть каждый раз отбрасываем одну переменную и обучаем модель оставшимся «n-1» переменным,
        4)  мы определяем переменную, удаление которой дало наименьшее (или нет) изменение производительности модели, а затем отбрасываем эту переменную,
        5) повторям этот процесс, пока ни одна переменная не может быть отброшена.
Нам нужно указать алгоритм и количество объектов, которые нужно выбрать, и мы вернемся к списку переменных, полученных при удалении объектов назад. Мы также можем проверить ранжирование переменных.

Выбор вперед (Forward Feature Elimination) - Это противоположный процесс обратного удаления признаков, который мы видели выше. Вместо того, чтобы исключать признакы, мы пытаемся найти лучшие признакы, которые улучшают производительность модели. Эта техника работает следующим образом:
    1) мы начинаем с одной функции. По сути, мы обучаем модель «n» раз, используя каждый признак отдельно,
    2) переменная, дающая наилучшую производительность, выбирается в качестве начальной переменной,
    3) затем мы повторяем этот процесс и добавляем одну переменную за раз,
    4) переменная, которая дает наибольшее увеличение производительности, сохраняется,
    5) мы повторяем этот процесс до тех пор, пока не будут замечены значительные улучшения в производительности модели.
ПРИМЕЧАНИЕ. Как обратное удаление, так и прямой выбор функций требуют много времени и вычислительных затрат.

Алгоритмы уменьшения размерности – анализ главных компонентов (Principal Component Analysis) PCA, это методика, которая помогает нам извлекать новый набор переменных из существующего большого набора переменных. 

Эти вновь извлеченные переменные называются главными компонентами. Вот некоторые из ключевых моментов, которые следует знать о PCA:
    1) главный компонент представляет собой линейную комбинацию исходных переменных,
    2) основные компоненты извлекаются таким образом, что первый основной компонент объясняет максимальную дисперсию в наборе данных,
    3) второй основной компонент пытается объяснить оставшуюся дисперсию в наборе данных и не связан с первым главным компонентом,
    4) каждое дополнительное измерение, которое мы добавляем к методике PCA, отражает все меньше и меньше дисперсии в модели. Первый компонент является наиболее важным, затем следует второй, затем третий и т. д.
    5) чтобы найти каждый компонент, алгоритм пытается максимизировать дисперсию, и каждый новый компонент должен быть ортогональным к другим.
Максимизирует дисперсию: определяет направление / размер максимальной дисперсии данных.
Ортогональный: находит направления, которые ортогональны (перпендикулярно первому найденному компоненту).

![](assets/ml_01.png)
**Анализ главных компонентов**

Слева мы имеем представление простого двумерного набора данных с тремя одномерными гиперплоскостями. С другой стороны, справа показан результат проецирования набора данных на каждую из этих одномерных гиперплоскостей. Глядя на эти три, становится ясно, что гиперплоскость, представленная сплошной линией, является той, которая максимизирует дисперсию по желанию. Поэтому сплошная линия является наиболее разумным выбором для более низкого измерения. Он также находит вторую ось (пунктирную линию), ортогональную первой, которая учитывает наибольшее количество оставшихся отклонений данных.

Если бы мы имели дело с более высокими измерениями, PCA нашел бы более ортогональную ось к предыдущим осям; столько осей, сколько число измерений в наборе данных. Единичный вектор, который определяет i-ю ось, называется главным компонентом (PC). В этом случае первый PC - это С1, а второй PC - С2.
после того, как мы определили наши основные компоненты, пришло время уменьшить размерность набора данных до d измерений, проецируя его на гиперплоскость, определенную первыми d основными компонентами.
Обычно большое количество измерений, это то, что составляет достаточно большую часть дисперсии (~ 90%).

Независимый компонентный анализ (Independent Component Analysis) ICA - основан на теории информации, а также является одним из наиболее широко используемых методов уменьшения размерности. Основное различие между PCA и ICA состоит в том, что PCA ищет некоррелированные факторы, в то время как ICA ищет независимые факторы.
Если две переменные некоррелированы, это означает, что между ними нет линейной зависимости. Если они независимы, это означает, что они не зависят от других переменных. Например, возраст человека не зависит от того, что он ест, или от того, сколько он смотрит телевизор.

Этот алгоритм предполагает, что данные переменные представляют собой линейные смеси некоторых неизвестных скрытых переменных. Также предполагается, что эти скрытые переменные являются взаимно независимыми, то есть они не зависят от других переменных и, следовательно, их называют независимыми компонентами наблюдаемых данных.

Факторный анализ (Factor Analysis) –  предположим, у нас есть две переменные: доход и образование. Эти переменные потенциально могут иметь высокую корреляцию, поскольку люди с более высоким уровнем образования, как правило, имеют значительно более высокий доход, и наоборот.
В методе факторного анализа переменные сгруппированы по их корреляциям, то есть все переменные в определенной группе будут иметь высокую корреляцию между собой, но низкую корреляцию с переменными другой группы (групп). Здесь каждая группа известна как фактор. Эти факторы невелики по сравнению с исходными размерами данных. Однако эти факторы трудно наблюдать.

Многообразное обучение или нелинейное уменьшение размерности (Manifold Learning) –  это как нелинейная версия PCA. PCA ищет плоские поверхности для описания данных. Если плоской поверхности не существует, мы используем Mainfold Learning, чтобы попытаться решить эту проблему более эффективно.
Существует много подходов для решения этой проблемы, таких как Isomap, Локально линейное вложение, Лапласово собственное отображение, Полуопределенное вложение и т. Д. Эти алгоритмы работают для извлечения низкоразмерного многообразия, которое можно использовать для описания многомерных данных.

Локально линейное вложение (LLE) –  это метод коллективного обучения, который не опирается на такие проекции, как PCA. Он работает, изучая, как каждое наблюдение линейно связано с его ближайшими соседями, а затем ищет низкоразмерное представление обучающего набора, где эти отношения лучше всего сохраняются. Для случаев, когда нет большого шума, он очень хорош при развертывании скрученных коллекторов.
t- распределенное стохастическое вложение соседей (t-SNE t- Distributed Stochastic Neighbor Embedding) –  t-SNE ищет шаблоны нелинейным способом. t-SNE - это один из немногих алгоритмов, который способен одновременно сохранять как локальную, так и глобальную структуру данных. Он рассчитывает вероятностное сходство точек в многомерном пространстве, а также в низкоразмерном пространстве. Эвклидовы расстояния больших размеров между точками данных преобразуются в условные вероятности, которые представляют сходства.
UMAP –  t-SNE очень хорошо работает с большими наборами данных, но также имеет свои ограничения, такие как потеря крупномасштабной информации, медленное время вычислений и неспособность осмысленно представлять очень большие наборы данных. Унифицированная аппроксимация и проекция многообразия (UMAP) - это метод сокращения размеров, который может сохранить как большую часть локальной, так и большей глобальной структуры данных по сравнению с t-SNE, с более коротким временем выполнения. 

Некоторые из ключевых преимуществ UMAP:

    • он может обрабатывать большие наборы данных и данных большого размера без особых проблем,
    • он сочетает в себе возможности визуализации с возможностью уменьшения размеров данных,
    • наряду с сохранением локальной структуры, он также сохраняет глобальную структуру данных. UMAP отображает близкие точки на многообразии в соседние точки в низкоразмерном представлении и делает то же самое для удаленных точек,
    • этот метод использует концепцию k-ближайшего соседа и оптимизирует результаты с использованием стохастического градиентного спуска. Сначала он вычисляет расстояние между точками в многомерном пространстве, проецирует их на низкоразмерное пространство и вычисляет расстояние между точками в этом низкоразмерном пространстве. Затем он использует Stochastic Gradient Descent, чтобы минимизировать разницу между этими расстояниями.

Корреляция между компонентами, полученными из UMAP, значительно меньше по сравнению с корреляцией между компонентами, полученными из t-SNE. Следовательно, UMAP имеет тенденцию давать лучшие результаты.
Краткое описание того, когда использовать каждую методику уменьшения размерности

Кратко подведем итоги использования каждого метода уменьшения размерности, который мы рассмотрели. Важно понимать, где использовать определенную технику, поскольку это помогает сэкономить время, усилия и вычислительные мощности.

![](assets/ml_02.png)
**Использование каждого метода уменьшения размерности**

Соотношение пропущенных значений: если в наборе данных слишком много пропущенных значений, мы используем этот подход для уменьшения количества переменных. Мы можем отбросить переменные с большим количеством пропущенных значений

Фильтр низкой дисперсии: мы применяем этот подход для определения и удаления постоянных переменных из набора данных. На целевую переменную не влияют чрезмерно переменные с низкой дисперсией, и, следовательно, эти переменные могут быть безопасно отброшены

Фильтр высокой корреляции: пара переменных, имеющих высокую корреляцию, увеличивает мультиколлинеарность в наборе данных. Таким образом, мы можем использовать эту технику, чтобы найти сильно коррелированные функции и отбросить их соответственно.

Случайный лес: это один из наиболее часто используемых методов, который говорит нам о важности каждого атрибута, присутствующего в наборе данных. Мы можем найти важность каждого атрибута и сохранить самые верхние атрибуты, что приведет к уменьшению размерности
Как методы обратного удаления, так и прямого выбора элементов занимают много вычислительного времени и поэтому обычно используются в небольших наборах данных.

Факторный анализ: этот метод лучше всего подходит для ситуаций, когда у нас есть сильно коррелированный набор переменных. Он делит переменные на основе их соотношения на разные группы и представляет каждую группу с коэффициентом

Анализ основных компонентов: это один из наиболее широко используемых методов работы с линейными данными. Он делит данные на набор компонентов, которые пытаются объяснить как можно больше различий
Независимый анализ компонентов: мы можем использовать ICA для преобразования данных в независимые компоненты, которые описывают данные с использованием меньшего количества компонентов
ISOMAP: мы используем эту технику, когда данные сильно нелинейны
t-SNE: этот метод также хорошо работает, когда данные сильно нелинейны. Это работает очень хорошо для визуализаций, а также
UMAP: эта методика хорошо работает для многомерных данных. Время его выполнения короче по сравнению с t-SNE.

Разделение набора данных (Dataset splitting) – набор данных, используемый для машинного обучения, должен быть разделен на три подмножества - наборы обучения, тестирования и проверки.

Обучающий набор: используется для обучения модели и определения ее оптимальных параметров - параметров, которые он должен изучить из данных.

Тестовый набор: тестовый набор необходим для оценки обученной модели и ее способности к обобщению. Обобщение означает способность модели идентифицировать закономерности в новых невидимых данных после того, как они прошли обучение по данным обучения. Крайне важно использовать различные подмножества для обучения и тестирования, чтобы избежать переобучения модели, что является неспособностью к обобщению, о котором мы упоминали выше.

Набор проверки: цель набора проверки состоит в том, чтобы настроить гиперпараметры модели - структурные параметры более высокого уровня, которые не могут быть непосредственно изучены из данных. Эти параметры могут указывать, например, насколько сложна модель и как быстро она находит шаблоны в данных.

Соотношение обучения и тестового набора обычно составляет 80 процентов. Затем обучающий набор снова разделяется, и его 20 процентов будут использоваться для формирования проверочного набора.
Чем больше используемых данных обучения, тем лучше будет работать потенциальная модель. Следовательно, больше используемых данных тестирования приводит к лучшей производительности модели и возможности обобщения.



