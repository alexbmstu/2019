****
# День 3. Алгоритмы машинного обучения <a name="4"></a>

## Методология машинного обучения <a name="4_1"></a>

Существует мнение о том, что не существует единого метода машинного обучения, который лучше всего справлялся бы со всеми проблемами. Независимо от того, насколько сложен или прост применяемый метод или алгоритм, он не будет работать наилучшим образом для всех проблем. Поэтому, чтобы найти наилучший метод и его алгоритмическую реализацию, которые соответствовали бы потребностям конкретной задачи, необходимо обладать широким кругозором методов и алгоритмов, а также владеть техническими средствами анализа данных.

В нашем конкурсе будем придерживаться следующей упрощенной методологии машинного обучения:

    - Прежде чем углубляться в сложные методы и тратить время на тонкую настройку модели, лучше попробовать более простые методы и алгоритмы. По мере продвижения к более сложным методам мы можем обнаружить, что для  наших нужд оказывается достаточно уже примененнного и более простого подхода.

    - Гибкая методология машинного обучения предполагают, что мы должны развивать понимание данных итерационно. Это означает, что мы не должны пытаться разрешить все проблемы сразу. Для науки о данных это означает, что мы начинаем с простого подхода и готовимся к применению более сложных методов, методик, алгоритмов, моделей и т.д и т.п.. 

    - Первая итерация должна иметь наиболее простой вариант реализации каждого этапа конвейера машинного обучения(например, обработка, извлечение признаков и пр.). Дополнительным достоинством такого подхода является то, что применение упрощенных подходов менее затратно с точки зрения вычислительной мощьности, не требует интенсивных вычислений или дорогих поисков гиперпараметров. Бесспорно, простая модель может работать плохо, но получение этой модели возможно максимально быстро и с минимальными затратами ресурсов. 

    - Для интерпретации полученных результатов и уточнения применяемых методов необходимо привлечение специалистов по предметной области, которые могут интерпретировать полученный результат. 

Пример такого простого метода: поиск ближайшего соседа и другие подобные методы. Для их реализации требуется лишь несколько строк кода. Вместе с тем нет никаких причин, по которым более простые методы не могут быть лучше сложных. На последующих итерациях конверйера машинного обучения исследуются другие подходы. Это позволит нам сравнить, как методологические изменения влияют на производительность, и отслеживать улучшения с течением времени. Тем не менее, по мере дальнейших исследований улучшение качества модели становится все более проблематичным. Например, если мы достигли точности 99%, возможно, нам не следует тратить больше времени и ресурсов, чтобы пробовать доводить ее до 99,2%. 

Далее подробнее рассмотрим этапы конвейера машинного обучения.


## Предварительная обработка данных <a name="4_2"></a>


Целью предварительной обработки является преобразование необработанных данных в форму, которая подходит для машинного обучения. Структурированные и чистые данные позволяют получать более точные результаты из прикладной модели. Этап предполагает форматирование данных, очистку и выборку достоверных значений. Этот этап может также включать в себя сокращение числа несвязанных признаков с помощью композиции признаков, если к исследованиям подключены специалисты, которые могут это сделать (для этого требуются расширенные знания предметной области). Привлечение специалистов в преметной области является важнейшим фактором успеха применения машинного обучения. 


### Очистка данных <a name="4_2_1"></a>

*Очистка данных*, это набор процедур, которые позволяют удалить шум и устранить несоответствия в данных. Этот процесс включает в себя заполнение недостающих данных с использованием метдов подстановки правдоподобных значений (таких, как замена отсутствующих значений на средние или максимальные значения и т.д.). Обнаружение выбросов (наблюдений, которые значительно отличаются от остальной части распределения) также важно для понимания данных. Анализ таких данных и принятие решения об очистке должен приниматься совместно со специалистом по предметной области.  Если какие-либо выбросы указывают на ошибочные данные, их следует удалить или исправить, если это возможно. Этот этап также включает в себя удаление неполных и бесполезных записей данных и столбцов.

### Масштабирование <a name="4_2_2"></a>

*Масштабирование* – данные могут иметь числовые атрибуты (признаки), которые охватывают сильно отличающиеся диапазоны, например, миллиметры, метры и километры. Масштабирование - это преобразование таких атрибутов таким образом, чтобы они имели одинаковый масштаб, например, в диапазоне от 0 до 1 или от 1 до 10 для наименьшего и наибольшего значения для атрибута. 

*Минимаксная* нормализация представляет собой линейное отображение данных из одного интервала в другой. Данный подход предполагает вычитание минимального значения объекта из остальных значений, после чего значения делятся на полученный диапазон. Он сохраняет исходное распределение функции и не вносит существенных изменений в информацию, встроенную в исходные данные. 

*Нормализация стандартным отклонением* модифицирует признаки путем вычитания среднего значения, а затем деления на стандартное отклонение. Это изменяет распределение признаков и приводит к распределению со средним значением, равным нулю, и стандартным отклонением, равным единице. 

*Логарифмическое преобразование* важно для преобразования мультипликативных отношений между признаками в аддитивные отношения. Так как большие значения уменьшаются больше, чем маленькие, логарифмическое преобразование имеет эффект псевдоскейлинга, поскольку различия между большими и малыми значениями в наборе данных уменьшаются. 

Выбор одного метода масштабирования среди других зависит не только от набора данных, но и от выбранного метода машинного обучения, поскольку различные методы машинного обучения фокусируются на разных аспектах данных. Например, метод кластеризации фокусируется на анализе сходства точек данных, в то время как анализ главных компонентов (PCA) выявляет наиболее существенные признаки данных. Использование наиболее адекватного метода масштабирования может улучшить результаты кластеризации.

Дополнительные источники литературы по данному разделу:

- [Практическое руководство по подготовке данных (en)](https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/)

- [Масштабирование данных данных (en)](https://codefellows.github.io/sea-python-401d5/lectures/rescaling_data.html#)


### Уменьшение размерности<a name="4_2_3"></a>

*Уменьшение размерности* (Dimesion Reduction) – основная проблема при интерпретации многомерных данных заключается в том, что соответствующие истинные закономерности скрыты нерелевантными данными или шумом. Для малых и средних наборов данных можно использовать классические статистические методы, которые фокусируются на идентификации сильных статистических закономеростей. Такие методы, однако, не могут быть распространены на случаи, когда размерность данных намного превышает размер записей, а слабые истинные сигналы окружены значительным количеством шума (например, как в случае современного геномного анализа). Кроме того, диапазон шума имеет тенденцию увеличиваться с увеличением размерности данных, что часто делает существующие методы непрактичными [3]. По мере роста количества признаков или измерений, объем данных, которые нам необходимы для точного обобщения, растет в геометрической прогрессии.

При добавлении нового признака в модель иногда не хватает данных для поддержания отношений, и, следовательно, новый признак может не иметь положительное влияние на модель. Например, в нашем исследовании мы имеем 299 переменных (p = 299). В этом случае мы можем иметь 299 (299-1) / 2 = 44551 различных парных групп признаков. Нет смысла визуализировать каждый из них в отдельности. В тех случаях, когда у нас большое количество переменных, лучше выбрать подмножество этих переменных (p << 100), которое собирает столько информации, сколько и исходный набор переменных. Вот некоторые преимущества применения уменьшения размерности к набору данных:

• пространство, необходимое для хранения данных, уменьшается по мере уменьшения количества измерений;

• уменьшение размеров приводит к меньшему времени вычислений / обучения;

• некоторые алгоритмы плохо обрабатывают большие размеры данных. Таким образом, чтобы эти алгоритмы были полезны, необходимо уменьшить размерность задачи;

• уменьшение размерности способствует т.н. мультиколлинеарности (наличии линейной зависимости между объясняющими переменными), удаляя лишние признаки. Например, у вас есть две переменные - «время, проведенное на беговой дорожке в минутах» и «потраченные калории». Эти переменные сильно коррелируют: чем больше времени вы проводите на беговой дорожке, тем больше калорий вы будете сжигать. Следовательно, нет смысла хранить оба, так как достаточно одного из них для создания адекватной модели.

• это помогает в визуализации данных. Как обсуждалось ранее, очень трудно визуализировать данные для многих измерений. Поэтому сокращение пространства до 2D или 3D может позволить нам более четко отображать и наблюдать кластеры данных.

Уменьшение размерности может быть сделано двумя различными способами:

• выбором признаков: сохраняются только самые важные переменные из исходного набора данных,

• путем нахождения меньшего набора новых переменных, каждая из которых является комбинацией входных переменных, содержащих в основном ту же информацию, что и входные переменные.


Дополнительные источники литературы по данному разделу:

- [Практическое руководство по подготовке данных (en)](https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/)

- [Масштабирование данных данных (en)](https://codefellows.github.io/sea-python-401d5/lectures/rescaling_data.html#)


### Алгоритм отбора признаков<a name="4_2_4"></a>

*Алгоритм отбора признаков* – один из наиболее широко используемых алгоритмов выбора объектов для построения модели данных. Этот алгоритм помогает выбрать меньшее подмножество признаков по сравнению с первоначальным. Алгоритм отбора признаков можно рассматривать как комбинацию техник поиска для представления нового поднабора признаков вместе с вычислением меры, которая отражает различие подмножеств признаков. 

    Wiki цитата: В традиционной статистике наиболее популярной формой отбора признаков является ступенчатая регрессия, которая является техникой оборачивания. Это жадный алгоритм, который добавляет лучший признак (или удаляет худший) на каждом шаге алгоритма. Главная проблема — когда остановить алгоритм. При обучении машин это обычно делается путём перекрёстной проверки. В статистике некоторые критерии оптимизированы. Это ведёт к наследованию проблемы вложения. Исследовались и более устойчивые методы, такие как метод ветвей и границ и кусочно-линейная сеть.

Дополнительные источники литературы по данному разделу:

[Алгоритмы отбора признаков (ру)](https://ru.wikipedia.org/wiki/%D0%9E%D1%82%D0%B1%D0%BE%D1%80_%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%B2)

### Обратное удаление признаков<a name="4_2_5"></a>

*Обратное удаление признаков* (Backward Feature Elimination) - рекурсивно удаляет некоторые отобранные признаки, строит модель с использованием оставшихся признаков и вычисляет точность модели. Он включает в себя следующие этапы:

1) сначала берутся все «n» переменных, присутствующих в наборе данных, и на них обучатся модель;

2) далее рассчитывается точность модели,

3) затем вычисляется точность модели после исключения каждой из переменной (всего получается n упрощенных моделей), то есть каждый раз отбрасывается одна переменная и модель строится на оставшихся «n-1» переменных;

4) определяется переменная, удаление которой дало наименьшее изменение точности модели, а затем эта переменная отбрасывается;

5) весь процесс повторяется пока какая-либо переменная может быть отброшена.

Для работы алгоритма нужно указать алгоритм и количество объектов, которые нужно выбрать. С помощью данного алгоритма также может быть выполнено ранжирование переменных.


### Выбор вперед<a name="4_2_6"></a>

*Выбор вперед* (Forward Feature Elimination) - это противоположный *обратному удалению признаков* процесс. Вместо того, чтобы исключать признаки и удалять их, мы пытаемся найти лучшие признаки для представления более точной модели. Эта техника работает следующим образом:

1) мы начинаем с одного признака. По сути, мы обучаем модель «n» раз, используя каждый признак отдельно;

2) переменная, дающая наилучшую точность обученной модели выбирается в качестве начальной переменной;

3) затем мы повторяем этот процесс и добавляем одну переменную за каждый проход алгоритма;

4) переменная, которая дает наибольшее увеличение производительности, фиксируется в модели;

5) мы повторяем этот процесс до тех пор, пока не будут замечены значительные улучшения в производительности модели.

    ПРИМЕЧАНИЕ. Как обратное удаление, так и прямой выбор функций требуют много времени и вычислительных затрат.

Дополнительные источники литературы по данному разделу:

[Forward Feature Elimination Wiki](https://en.wikipedia.org/wiki/Feature_selection)

### Анализ главных компонентов<a name="4_2_7"></a>

*Анализ главных компонентов* (Principal Component Analysis) PCA, это методика, которая помогает нам извлекать новый набор переменных из существующего большого набора переменных. Эти вновь извлеченные переменные и называются главными компонентами. 

Вот некоторые из ключевых моментов, которые следует знать о PCA:

1) главные компоненты представляет собой линейную комбинацию исходных переменных;

2) основные компоненты выбираются таким образом, что первый основной компонент обеспечивал максимальную дисперсию в наборе данных;

3) второй основной компонент пытается объяснить оставшуюся дисперсию в наборе данных и не связан с первым главным компонентом,

4) каждое дополнительное измерение, которое мы добавляем в методике PCA, отражает все меньше и меньше дисперсии в модели. Первый компонент является наиболее важным, затем следует второй, затем третий и т. д.

5) чтобы найти каждый компонент, алгоритм пытается максимизировать дисперсию, и каждый новый компонент должен быть ортогональным к другим.

![](assets/ml_01.png)
**Анализ главных компонентов**

Слева мы имеем представление простого двумерного набора данных с тремя одномерными гиперплоскостями. С другой стороны, справа показан результат проецирования набора данных на каждую из этих одномерных гиперплоскостей. Cтановится ясно, что гиперплоскость, представленная сплошной линией, выявляет максимальную дисперсию в наборе данных. Она также позволяет найти вторую ось (пунктирную линию), ортогональную первой, которая учитывает наибольшее количество оставшихся отклонений данных.

Если бы мы имели дело с большим количеством измерений (как это и есть в реальных задачах), PCA нашел бы большее количество ортогональных осей к предыдущим осям (фактически столько осей, каково число измерений в наборе данных). Единичный вектор, который определяет i-ю ось, называется главным компонентом (PC). В этом случае первый PC - это С1, а второй PC - С2. После того, как мы определили наши основные компоненты, пришло время уменьшить размерность набора данных до d измерений, проецируя его на гиперплоскость, определенную первыми d основными компонентами. Обычно, в практических задачах, большое количество измерений, это то, что составляет достаточно большую часть дисперсии (~ 90%).

Дополнительные источники литературы по данному разделу:

[Метод главных компонент (Wiki)](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82)


### Независимый компонентный анализ<a name="4_2_8"></a>


*Независимый компонентный анализ* (Independent Component Analysis, ICA) - основан на теории информации, а также является одним из наиболее широко используемых методов уменьшения размерности. Основное различие между PCA и ICA состоит в том, что PCA ищет некоррелированные факторы, в то время как ICA ищет независимые факторы. Если две переменные некоррелированы, это означает, что между ними нет линейной зависимости. Если они независимы, это означает, что они не зависят от других переменных. Например, возраст человека не зависит от того, что он ест, или от того, сколько он смотрит телевизор.

Этот алгоритм предполагает, что данные переменные представляют собой линейные смеси некоторых неизвестных скрытых переменных. Также предполагается, что эти скрытые переменные являются взаимно независимыми, то есть они не зависят от других переменных и, следовательно, их называют независимыми компонентами наблюдаемых данных.

Дополнительные источники литературы по данному разделу:

[Анализ независимых компонент Wiki](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D0%BD%D0%B5%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D0%BC%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82)


### Факторный анализ<a name="4_2_9"></a>


*Факторный анализ* (Factor Analysis) –  предположим, у нас есть две переменные: доход и образование. Эти переменные потенциально могут иметь высокую корреляцию, поскольку люди с более высоким уровнем образования, как правило, имеют значительно более высокий доход, и наоборот. В методе факторного анализа переменные сгруппированы по их корреляциям, то есть все переменные в определенной группе будут иметь высокую корреляцию между собой, но низкую корреляцию с переменными другой группы (групп). Здесь каждая группа известна как фактор. Эти факторы невелики по сравнению с исходными размерами данных. Однако эти факторы трудно наблюдать.

[Факторный анализ](https://ru.wikipedia.org/wiki/%D0%A4%D0%B0%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7)
[Просто о факторном анализе](https://habr.com/ru/post/224495/)


### Многообразное обучение или нелинейное уменьшение размерности<a name="4_2_10"></a>


*Многообразное обучение или нелинейное уменьшение размерности* (Manifold Learning) –  это нелинейная версия PCA. Проблема состоит в том, что PCA ищет плоские поверхности для описания данных, которые не всегда наилучшим образом показывают макимальную дисперсию. Если плоской поверхности не существует, мы используем Mainfold Learning, чтобы попытаться решить эту проблему более эффективно.
Существует много подходов для решения этой проблемы, таких как Isomap, *Локально линейное вложение*, *Лапласово собственное отображение*, *Полуопределенное вложение* и т.д. Эти алгоритмы работают для извлечения низкоразмерного многообразия, которое можно использовать для описания многомерных данных.


### Локально линейное вложение<a name="4_2_11"></a>

*Локально линейное вложение* (LLE, Locally-Linear Embedding) –  это метод коллективного обучения, который не опирается на проекции в гиперплоскости, подобно PCA. Он работает, изучая, как каждое наблюдение линейно связано с его ближайшими соседями, а затем ищет низкоразмерное представление обучающего набора, где эти отношения лучше всего сохраняются. Для случаев, когда нет большого шума, LLE показывает лучшие результаты по сравнению с PCA. 

[An Introduction to Locally Linear Embedding](https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf)

### Cтохастическое вложение соседей с t-распределением<a name="4_2_12"></a>

*Cтохастическое вложение соседей с t-распределением* (t-SNE, t- Distributed Stochastic Neighbor Embedding) –  ищет шаблоны нелинейным способом. t-SNE - это один из немногих алгоритмов, который способен одновременно сохранять как локальную, так и глобальную структуру данных. Он рассчитывает вероятностное сходство точек в многомерном пространстве, а также в низкоразмерном пространстве. Эвклидовы расстояния больших размеров между точками данных преобразуются в условные вероятности, которые представляют сходства.
UMAP –  t-SNE очень хорошо работает с большими наборами данных, но также имеет свои ограничения, такие как потеря крупномасштабной информации, медленное время вычислений и неспособность осмысленно представлять очень большие наборы данных. 

[t-SNE Вики](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%B2%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9_%D1%81_t-%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC)

[Алгоритм t-SNE. Иллюстрированный вводный курс](http://datareview.info/article/algoritm-t-sne-illyustrirovannyiy-vvodnyiy-kurs/)

### Унифицированная аппроксимация и проекция многообразия<a name="4_2_13"></a>

*Унифицированная аппроксимация и проекция многообразия* (UMAP) - это метод сокращения размерности, который может сохранить как большую часть локальной, так и большей глобальной структуры данных по сравнению с t-SNE, с более коротким временем выполнения. 

Некоторые из ключевых преимуществ UMAP:

• он может обрабатывать большие наборы данных и данных большого размера без особых проблем,

• он сочетает в себе возможности визуализации с возможностью уменьшения размеров данных,

• наряду с сохранением локальной структуры, он также сохраняет глобальную структуру данных. UMAP отображает близкие точки на многообразии в соседние точки в низкоразмерном представлении и делает то же самое для удаленных точек,

• этот метод использует концепцию k-ближайшего соседа и оптимизирует результаты с использованием стохастического градиентного спуска. Сначала он вычисляет расстояние между точками в многомерном пространстве, проецирует их на низкоразмерное пространство и вычисляет расстояние между точками в этом низкоразмерном пространстве. Затем он использует Stochastic Gradient Descent, чтобы минимизировать разницу между этими расстояниями.

Корреляция между компонентами, полученными из UMAP, значительно меньше по сравнению с корреляцией между компонентами, полученными из t-SNE. Следовательно, UMAP имеет тенденцию давать лучшие результаты.
Краткое описание того, когда использовать каждую методику уменьшения размерности

[Uniform Approximation and Projection (UMAP) Вики](https://ru.wikipedia.org/wiki/UMAP)



### Краткий обзор алгоритмов и методов уменьшения размерности <a name="4_2_14"></a>
Кратко подведем итоги использования каждого метода уменьшения размерности, который мы рассмотрели. Важно понимать, где использовать определенную технику, поскольку это помогает сэкономить время, усилия и вычислительные мощности.

![](assets/ml_02.png)
**Использование методов уменьшения размерности**

#### Соотношение пропущенных значений<a name="4_2_14_1"></a>

Если в наборе данных слишком много пропущенных значений, мы используем этот подход для уменьшения количества переменных. Мы можем отбросить переменные с большим количеством пропущенных значений.

#### Фильтр низкой дисперсии<a name="4_2_14_2"></a>

Мы применяем этот подход для определения и удаления постоянных переменных из набора данных. На целевую переменную не влияют чрезмерно переменные с низкой дисперсией, и, следовательно, эти переменные могут быть безопасно отброшены

#### Фильтр высокой корреляции<a name="4_2_14_3"></a>

Пара переменных, имеющих высокую корреляцию, увеличивает мультиколлинеарность в наборе данных. Таким образом, мы можем использовать эту технику, чтобы найти сильно коррелированные функции и отбросить их соответственно.

#### Случайный лес<a name="4_2_14_4"></a>

Это один из наиболее часто используемых методов, который говорит нам о важности каждого атрибута, присутствующего в наборе данных. Мы можем найти важность каждого атрибута и сохранить самые верхние атрибуты, что приведет к уменьшению размерности. Как методы обратного удаления, так и прямого выбора элементов занимают много вычислительного времени и поэтому обычно используются в небольших наборах данных.

#### Факторный анализ<a name="4_2_14_5"></a>

Этот метод лучше всего подходит для ситуаций, когда у нас есть сильно коррелированный набор переменных. Он делит переменные на основе их соотношения на разные группы и представляет каждую группу с коэффициентом

#### Анализ основных компонентов<a name="4_2_14_6"></a>

Это один из наиболее широко используемых методов работы с линейными данными. Он делит данные на набор компонентов, которые пытаются объяснить как можно больше различий
Независимый анализ компонентов: мы можем использовать ICA для преобразования данных в независимые компоненты, которые описывают данные с использованием меньшего количества компонентов

- ISOMAP: мы используем эту технику, когда данные сильно нелинейны;

- t-SNE: этот метод также хорошо работает, когда данные сильно нелинейны или же требуется более понятная визуализаци данных.

- UMAP: эта методика хорошо работает для многомерных данных. Время его выполнения короче по сравнению с t-SNE.

### Разделение набора данных<a name="4_2_20"></a>

**Разделение набора данных** (Dataset splitting) – набор данных, используемый для машинного обучения, должен быть разделен на три подмножества - наборы обучения, тестирования и проверки.

**Обучающий набор**: используется для обучения модели и определения ее оптимальных параметров - параметров, которые он должен изучить из данных.

**Тестовый набор**: тестовый набор необходим для оценки обученной модели и ее способности к обобщению. Обобщение означает способность модели идентифицировать закономерности в новых невидимых данных после того, как они прошли обучение по данным обучения. Крайне важно использовать различные подмножества для обучения и тестирования, чтобы избежать переобучения модели, что является неспособностью к обобщению, о котором мы упоминали выше.

**Валидационный набор**: цель выделения валидационный набор набора состоит в том, чтобы настроить гиперпараметры модели - структурные параметры более высокого уровня, которые не могут быть непосредственно изучены из данных. Эти параметры могут указывать, например, насколько сложна модель и как быстро она находит шаблоны в данных.

Соотношение обучения и тестового набора обычно составляет 80 процентов. Затем обучающий набор снова разделяется, и его 20 процентов будут использоваться для формирования валидационного набора.
Чем больше используемых данных обучения, тем лучше будет работать потенциальная модель. Следовательно, больше используемых данных тестирования приводит к лучшей производительности модели и возможности обобщения.



## Алгоритмы машинного обучения <a name="4_3"></a>

После того, как мы предварительно обработали собранные данные и разбили их на три подмножества, мы приступаем к обучению модели. Этот процесс влечет за собой снабжение алгоритма данными обучения. Затем алгоритм обрабатывает данные и выводит модель, которая может найти целевое значение (атрибут) в новых данных (ответ, который мы хотим получить с помощью прогнозного анализа). Целью обучения модели является разработка модели.

Наиболее распространены два модельных стиля обучения - обучение с учителем и обучение без учителя. Выбор каждого стиля зависит от того, должны ли мы прогнозировать конкретные атрибуты или группировать объекты данных по сходству.

• *обучение с учителем*: Контролируемое обучение позволяет обрабатывать данные с целевыми атрибутами или помеченными данными. Обучение с учителем решает проблемы классификации и регрессии.

• *обучение без учителя*: Во время этого стиля обучения алгоритм анализирует немеченые данные. Цель обучения модели - найти скрытые взаимосвязи между объектами данных и объектами структуры по сходствам или различиям. Обучение без учителя направлено на решение таких проблем, как кластеризация, обучение правилам ассоциаций и уменьшение размерности. Например, его можно применять на этапе предварительной обработки данных, чтобы уменьшить сложность данных.

Существует два других стиля обучения модели: *полу-контролируемый*, в котором набор данных содержит как помеченные, так и немаркированные примеры. Второй стиль назвается *обучение с подкреплением*, при котором машина пытается выучить политику действий в соответствии с получением вознаграждения за каждое действие.
Мы опишем алгоритмы, которые являются не только самыми известными, но и либо очень эффективными сами по себе, либо используются в качестве строительных блоков для самых эффективных алгоритмов обучения.


![](assets/ml_03.png)
**Алгоритмы машинного обучения**

###  Линейная регрессия<a name="4_3_1"></a>

*Линейная регрессия* –  метод поиска зависимости между входными и выходными переменными с линейной функцией связи. Один из способов вычислить значения параметров модели является метод наименьших квадратов (МНК), который минимизирует среднеквадратичную ошибку между реальным значением зависимой переменной и прогнозом, выданным моделью. 

Пусть мы хотим построить модель объясняемой (зависимой) переменной в виде линейной комбинации остальных (независимых) признаков. Цель алгоритма МНК – построить гиперплоскость, максимально приближенную ко всем обучающим примерам.

<img src="assets/ml_04.png" width="400">
**Линейная регрессия**

На рисунке показана линия регрессии (красным цветом) для одномерных примеров (синие точки). Мы можем использовать эту прямую, чтобы предсказать значение объясняемой переменной *y(new)* для нового значения независимой переменной *x(new)*.

Чтобы получить эту оптимальную линию (или гиперплоскость в n-мерном случае), процедура оптимизации пытается минимизировать следующее выражение *функции стоимости*:

<img src="assets/ml_f2.png" width="200">

Выражение под знаком суммы называется *функцией потерь*. Суммировав этy функции для всех значений признака и разделив на количество значений *N* мы получаем среднюю ошибку при выборе конкретной прямой, которая и является функцией стоимости для линейной регрессии. Параметры *w* и *b* для 2-мерной регрессии определяет наклон прямой и ее смещение (в выражении f(x) = w\*x + b). Для большей размерности *w* и *b* представляют собой вектора, определяющие гиперплоскость. В этом случае функция *f* принимает вид:

<img src="assets/ml_f3.png" width="270">



###  Деревья решений<a name="4_3_2"></a>

*Деревья решений* – мы обсудили деревья решений и их использование для уменьшения размерности. Это очень популярный и простой метод. Их графика помогает увидеть, что происходит, а их движок требует систематического, документированного мыслительного процесса [4].

Идея этого алгоритма довольно проста. В каждом узле мы выбираем наилучшее разделение всех объектов и всех возможных точек разделения. Каждый сплит выбирается таким образом, чтобы максимизировать какой-то функционал. В деревьях классификации мы используем перекрестную энтропию и индекс Джини. В деревьях регрессии мы минимизируем сумму квадратов ошибок между прогнозирующей переменной целевых значений точек, которые попадают в этот регион, и той, которую мы присваиваем ей.

Мы выполняем эту процедуру рекурсивно для каждого узла и завершаем работу, когда выполняем критерии остановки. Они могут варьироваться от минимального количества листьев в узле до высоты дерева. Отдельные деревья используются очень редко, но по составу, как и многие другие, они создают очень эффективные алгоритмы, такие как Случайный лес или повышение градиентного дерева (Рисунок 14).


![](assets/ml_05.png)
**Деревья решений**


###  Метод опорных векторов<a name="4_3_3"></a>

*Метод опорных векторов* (SVM – support vector machine) – он используется, когда в данных присутствует шум, и никакая гиперплоскость не может идеально отделить положительные примеры от отрицательных.
Для того, чтобы распространить на случаи, в которых данные не линейно разделимы, используется функция потерь шарнира:



Функция потери шарнира равна нулю, если лежит на правильной стороне границы решения. Для данных с неправильной стороны границы решения значение функции пропорционально расстоянию от границы решения. Затем мы пытаемся минимизировать следующую функцию стоимости:



где 	 – гиперпараметр, которая определяет компромисс между увеличением размера границы решения и обеспечением того, что каждый из них находится на правильной стороне границы решения. 
Значение C обычно выбирается экспериментально.

Действительно, если нам удастся преобразовать исходное пространство в пространство более высокой размерности, мы могли бы надеяться, что примеры станут линейно разделимыми в этом преобразованном пространстве. В SVM использование функции для неявного преобразования исходного пространства в пространство более высокого измерения во время оптимизации функции стоимости называется трюком ядра.


###  Алгоритм k-ближайших соседей<a name="4_3_4"></a>


KNN (k-ближайшие соседи) – алгоритм k-ближайших соседей использует весь набор данных в качестве обучающего набора, а не разделяет набор данных на обучающий набор и набор тестов.
Когда для нового экземпляра данных требуется результат, алгоритм KNN просматривает весь набор данных, чтобы найти k-ближайших экземпляров для нового экземпляра, или k экземпляров, наиболее похожих на новую запись, а затем выводит среднее значение результаты (для проблемы регрессии) или режим (наиболее частый класс) для задачи классификации. Значение k определяется пользователем.

Сходство между экземплярами рассчитывается с использованием таких мер, как евклидово расстояние и расстояние Хемминга.
Это самый четкий метод кластеризации, который все еще имеет некоторые недостатки. Прежде всего, мы должны знать количество кластеров. Во-вторых, результат зависит от точек, случайно выбранных в начале, и алгоритм не гарантирует, что мы достигнем глобального минимума функционала.


## Оценка и тестирование модели <a name="4_4"></a>

Цель этого этапа, разработать простейшую модель, способную быстро и достаточно хорошо сформулировать целевое значение. Эта цель достигнута с помощью модели тюнинга. Это оптимизация параметров модели для достижения максимальной производительности алгоритма.


###  Перекрестная проверка<a name="4_4_1"></a>

Одним из наиболее эффективных методов оценки и настройки модели является перекрестная проверка. *Перекрестная проверка* является наиболее часто используемым методом настройки. Это влечет за собой разделение учебного набора данных на десять равных частей (складок). Данная модель обучается только в девяти сгибах, а затем проверяется на десятой (ранее не учтенной). Тренировка продолжается до тех пор, пока каждая складка не будет оставлена ​​в стороне и использована для тестирования. В результате измерения производительности модели для каждого набора гиперпараметров рассчитывается перекрестная оценка. Модели обучаются с использованием различных наборов гиперпараметров, чтобы определить, какая модель имеет самую высокую точность прогнозирования. Перекрестно подтвержденный балл указывает на среднюю производительность модели по десяти сгибам удержания.
Затем мы тестируем модели с набором значений гиперпараметров, которые получили лучший перекрестно проверенный результат. Существуют различные метрики ошибок для задач машинного обучения.


###  Улучшение прогнозов с помощью методов ансамбля<a name="4_4_2"></a>

Улучшение прогнозов с помощью методов ансамбля – исследователи данных в основном создают и обучают одну или несколько десятков моделей, чтобы иметь возможность выбрать оптимальную модель среди хорошо работающих. Модели обычно показывают разные уровни точности, поскольку они допускают разные ошибки в новых точках данных. Есть способы улучшить аналитические результаты. Методы ансамбля моделей позволяют достичь более точного прогноза, используя несколько наиболее эффективных моделей и комбинируя их результаты. Точность, как правило, рассчитывается по средним ормедианским выходам всех моделей в ансамбле. Среднее значение - это общее количество голосов, поделенное на их количество. Медиана представляет собой средний балл для голосов, упорядоченных по размеру.

Распространенными методами ансамбля являются Stacking, Bagging и Boosting.

*Stacking* –  Этот подход, также известный как многоуровневое обобщение, предлагает разработку метамодели или ученика более высокого уровня путем объединения нескольких базовых моделей. Stacking обычно используются для объединения моделей различных типов. Цель этого метода – уменьшить ошибку обобщения.

*Bagging* –  (начальная загрузка). Это метод последовательного объединения моделей. Сначала обучающий набор данных разбивается на подмножества. Затем модели обучаются на каждом из этих подмножеств. После этого прогнозы объединяются с использованием среднего или большинства голосов. Bagging помогает уменьшить ошибку дисперсии и избежать модели переобучения.

*Boosting* –  Согласно этой методике, работа делится на два этапа. Сначала мы используем подмножества исходного набора данных, чтобы разработать несколько моделей со средней эффективностью, а затем объединяем их, чтобы повысить производительность, используя большинство голосов. Каждая модель обучается на подмножестве, полученном в результате исполнения предыдущей модели, и концентрируется на неправильно классифицированных записях.

Можно развернуть модель, которая наиболее точно прогнозирует значения результатов в тестовых данных.


##  Развертывание модели<a name="4_5"></a>


Развертывание модели (Model deployment) – этап развертывания модели включает в себя ввод модели в эксплуатацию.
После того, как мы выбрали надежную модель и определили ее требования к производительности, мы интегрируем модель с производственной средой.

Затем мы измеряем производительность модели с помощью A / B-тестирования. Тестирование может показать, как, например, число клиентов, работающих с моделью, используемой для персональной рекомендации, соотносится с бизнес-целью.

Рабочий процесс развертывания зависит от бизнес-инфраструктуры и проблемы, которую мы хотим решить. Прогностическая модель может быть ядром новой отдельной программы или может быть включена в существующее программное обеспечение.

Производительность модели также зависит от того, выполнили ли мы вышеупомянутые этапы (подготовка и предварительная обработка набора данных, моделирование) вручную с использованием собственной ИТ-инфраструктуры или автоматически с одним из машинного обучения в качестве сервисных продуктов.


*Пакетный прогноз* – Это вариант развертывания подходит, когда нам не нужны прогнозы на постоянной основе. Когда мы выбираем этот тип развертывания, мы получаем один прогноз для группы наблюдений. Модель обучается на статическом наборе данных и выводит прогноз. Развертывание не требуется, если необходим единый прогноз. Например, мы можем решить проблему классификации, чтобы узнать, принимает ли определенная группа клиентов предложение или нет.

*Веб-сервис* – такой рабочий процесс машинного обучения позволяет получать прогнозы практически в реальном времени. Модель, однако, обрабатывает одну запись из набора данных за раз и делает для нее прогнозы.

*Прогноз в реальном времени* (потоковое в реальном времени) – с помощью потоковой аналитики в реальном времени мы можем мгновенно анализировать потоковые данные в реальном времени и быстро реагировать на события, которые происходят в любой момент. Прогнозирование в реальном времени позволяет обрабатывать данные датчиков или рынка, данные из Интернета вещей или мобильных устройств, а также из мобильных или настольных приложений и веб-сайтов.







